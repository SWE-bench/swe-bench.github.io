{% extends 'base.html' %}

{% block title %}SWE-bench Bash Only{% endblock %}

{% block head_extra %}
    <link rel="icon" href="favicon.ico" type="image/x-icon">
{% endblock %}

{% block content %}
    <header class="page-header">
        <div class="container">
            <div class="d-flex align-center mb-3">
                <img src="img/swe-llama.svg" alt="SWE-Llama Logo" style="height: 5em; width: auto; margin-right: 1em;">
                <h1 class="mb-0">SWE-bench Bash Only</h1>
            </div>
            <p>Evaluating language models in minimal bash environments (<a href="https://github.com/SWE-agent/mini-swe-agent">mini-SWE-agent</a>) for fair LM comparison.</p>
            <br>
            <div class="flex gap-2">
                <a href="https://arxiv.org/abs/2310.06770" target="_blank" class="paper-link">Paper</a>
                <a href="https://github.com/SWE-agent/mini-swe-agent" target="_blank" class="paper-link">mini-SWE-agent</a>
                <a href="https://github.com/SWE-bench/SWE-bench" target="_blank" class="paper-link">GitHub</a>
            </div>
        </div>
    </header>

    <div class="container">
        <section class="content-box">
            <h2>Overview</h2>
            <p>The original SWE-bench benchmark aims to evaluate arbitrary systems on their ability to resolve GitHub issues. Currently, top-performing systems represent a wide variety of AI scaffolds; from simple LM agent loops, to RAG systems, to multi-rollout and review type systems. Each of these systems are totally valid solutions to the problem of solving GitHub issues.</p>

            <p>However, when we first created SWE-bench, we were initially interested in evaluating LMs primarily. To make an apples-to-apples comparison of LMs easier, we've introduced the <strong>SWE-bench Bash Only</strong> leaderboard. In this setting, we use our <a href="https://github.com/SWE-agent/mini-swe-agent">mini-SWE-agent</a> package to evaluate LMs in a minimal bash environment. No tools, no special scaffold structure; just a simple <a href="https://arxiv.org/abs/2210.03629">ReAct</a> agent loop. Results on SWE-bench Bash Only represent the state-of-the-art LM performance when given just a bash shell and a problem.</p>

            <details>
                <summary>Details</summary>

                <ul>
                    <li>We use <a href="https://github.com/SWE-agent/mini-swe-agent/blob/main/src/minisweagent/config/extra/swebench.yaml">this configuration</a> for all models.</li>
                    <li>The LM temperature is set to 0.0 if the temperature parameter is supported.</li>
                    <li><a href="https://mini-swe-agent.com/latest/usage/swebench/">This guide</a> shows how to run the evaluation yourself.</li>
                    <li>
                        Small changes in the setup and configuration are captured by the version number in the leaderboard. 
                        Version numbers correspond to tags in the mini-SWE-agent repository. 
                        Since the mini-SWE-agent repository contains other components as well, a new version number does not necessarily mean that anything of relevance has changed for the bash-only leaderboard setting.
                        We do <em>not</em> aim to tune the configuration and setup to reach higher and higher scores. 
                        Instead, we only make general fixes to the framework, as well as clarifications in the prompt to provide a maximally fair evaluation setup for the LMs. </li>
                </ul>

            </details>
        </section>

    </div>
    
    <div class="container">
        <section class="content-box">
            <h2>Leaderboard</h2>
            {% set page_context = "bash-only" %}
            {% include "_leaderboard_table.html" %}
        </section>
    </div>

    <div class="container">
        <section class="content-box" id="citation">
            <h2>Citation</h2>
            <p>If you use SWE-bench Bash Only in your research, please cite our paper:</p>
            
            <div class="citation-type">
                <button class="citation-format-btn active" data-format="bibtex" data-target="bash-only-citation">BibTeX</button>
                <button class="citation-format-btn" data-format="apa" data-target="bash-only-citation">APA</button>
                <button class="citation-format-btn" data-format="mla" data-target="bash-only-citation">MLA</button>
            </div>
            
            <div class="citation-container" id="bash-only-citation-bibtex">
                <button class="copy-btn" aria-label="Copy citation">Copy</button>
                <pre>@inproceedings{
    jimenez2024swebench,
    title={{"{"}}{{"{"}}SWE{{"}"}}-bench: Can Language Models Resolve Real-world Github Issues?},
    author={Carlos E Jimenez and John Yang and Alexander Wettig and Shunyu Yao and Kexin Pei and Ofir Press and Karthik R Narasimhan},
    booktitle={The Twelfth International Conference on Learning Representations},
    year={2024},
    url={https://openreview.net/forum?id=VTF8yNQM66}
}</pre>
            </div>
            
            <div class="citation-container display-none" id="bash-only-citation-apa">
                <button class="copy-btn" aria-label="Copy citation">Copy</button>
                <pre>Jimenez, C. E., Yang, J., Wettig, A., Yao, S., Pei, K., Press, O., & Narasimhan, K. R. (2024). SWE-bench: Can Language Models Resolve Real-world Github Issues? arXiv preprint arXiv:2310.06770.</pre>
            </div>
            
            <div class="citation-container display-none" id="bash-only-citation-mla">
                <button class="copy-btn" aria-label="Copy citation">Copy</button>
                <pre>Jimenez, Carlos E., et al. "SWE-bench: Can Language Models Resolve Real-world Github Issues?" arXiv preprint arXiv:2310.06770 (2024).</pre>
            </div>
        </section>
    </div>
{% endblock %}

{% block scripts_extra %}

{% endblock %} 