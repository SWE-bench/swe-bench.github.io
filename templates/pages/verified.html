{% extends 'base.html' %}

{% block title %}SWE-bench Verified{% endblock %}

{% block head_extra %}
    <link rel="icon" href="favicon.ico" type="image/x-icon">
{% endblock %}

{% block content %}
    <header class="page-header">
        <div class="container">
            <div class="d-flex align-center mb-3">
                <img src="img/swe-llama.svg" alt="SWE-Llama Logo" style="height: 5em; width: auto; margin-right: 1em;">
                <h1 class="mb-0">SWE-bench Verified</h1>
            </div>
            <p>A human-validated subset of 500 SWE-bench instances for reliable evaluation of coding agents and language models.</p>
            <br>
            <div class="flex gap-2">
                <a href="https://openai.com/index/introducing-swe-bench-verified/" target="_blank" class="paper-link">OpenAI Blog Post <i class="fas fa-external-link-alt"></i></a>
                <a href="https://arxiv.org/abs/2310.06770" target="_blank" class="paper-link">Paper</a>
                <a href="https://github.com/SWE-bench/SWE-bench" target="_blank" class="paper-link">GitHub</a>
            </div>
        </div>
    </header>

    <div class="container">
        <section>
            <h2>Overview</h2>
            <p>SWE-bench Verified is a human-filtered subset of 500 instances from SWE-bench, created in collaboration with OpenAI. Human annotators reviewed each instance to ensure the problem descriptions are clear, the test patches are correct, and the tasks are solvable given the available information. Read more in the <a href="https://openai.com/index/introducing-swe-bench-verified/" target="_blank">OpenAI blog post</a>.</p>

            <p>The Verified leaderboard features results from a wide variety of AI coding systems, from simple LM agent loops to RAG systems to multi-rollout and review type systems.</p>
        </section>

        <hr />

        <section>
            <h2>Bash Only: Comparing Language Models</h2>
            <p>While the full leaderboard compares arbitrary systems, we are also interested in evaluating language models directly. To make an apples-to-apples comparison of LMs easier, we evaluate all LMs using <a href="https://github.com/SWE-agent/mini-swe-agent">mini-SWE-agent</a> in a minimal bash environment. No tools, no special scaffold structure; just a simple <a href="https://arxiv.org/abs/2210.03629">ReAct</a> agent loop. These results represent the state-of-the-art LM performance when given just a bash shell and a problem.</p>

            <p>On the leaderboard, use the <strong>Agent</strong> dropdown to select between the mini-SWE-agent results and the full leaderboard with all agents.</p>

            <details>
                <summary>Click for more details on the bash-only setup</summary>
                <ul>
                    <li>We use <a href="https://github.com/SWE-agent/mini-swe-agent/blob/main/src/minisweagent/config/extra/swebench.yaml">this configuration</a> for all models.</li>
                    <li>The release number in the leaderboard corresponds to the version of the mini-SWE-agent used to run the evaluation.</li>
                    <li>Results of release 1.x and 2.x are not necessarily comparable to each other, as 2.x uses tool calling to invoke actions, whereas 1.x parses action from the output strings. Read more about the changes in the <a href="https://mini-swe-agent.com/latest/advanced/v2_migration/">mini-SWE-agent v2 migration guide</a>.</li>
                    <li>For all results of release 1.x and earlier, the LM temperature is set to 0.0 if the temperature parameter is supported. For all results of release 2.x and later, the temperature parameter is not set.</li>
                    <li>
                        Other than the aforementioned notes, small changes in the setup and configuration are captured by the version number in the leaderboard.
                        Version numbers correspond to tags in the mini-SWE-agent repository.
                        Since the mini-SWE-agent repository contains other components as well, a new version number does not necessarily mean that anything of relevance has changed for the bash-only leaderboard setting.
                        We do <em>not</em> aim to tune the configuration and setup to reach higher and higher scores.
                        Instead, we only make general fixes to the framework, as well as clarifications in the prompt to provide a maximally fair evaluation setup for the LMs.
                        Generally, everything in the minor or patch release version number should be a minor change for the purpose of the bash-only leaderboard.
                    </li>
                    <li><a href="https://mini-swe-agent.com/latest/usage/swebench/">This guide</a> shows how to run the evaluation yourself.</li>
                </ul>
            </details>
        </section>

        <hr />

        <section id="citation">
            <h2>Citation</h2>
            <p>If you use SWE-bench Verified in your research, please cite our paper:</p>

            <div class="citation-type">
                <button class="citation-format-btn active" data-format="bibtex" data-target="verified-citation">BibTeX</button>
                <button class="citation-format-btn" data-format="apa" data-target="verified-citation">APA</button>
                <button class="citation-format-btn" data-format="mla" data-target="verified-citation">MLA</button>
            </div>

            <div class="citation-container" id="verified-citation-bibtex">
                <button class="copy-btn" aria-label="Copy citation">Copy</button>
                <pre>@inproceedings{
    jimenez2024swebench,
    title={{"{"}}{{"{"}}SWE{{"}"}}-bench: Can Language Models Resolve Real-world Github Issues?},
    author={Carlos E Jimenez and John Yang and Alexander Wettig and Shunyu Yao and Kexin Pei and Ofir Press and Karthik R Narasimhan},
    booktitle={The Twelfth International Conference on Learning Representations},
    year={2024},
    url={https://openreview.net/forum?id=VTF8yNQM66}
}</pre>
            </div>

            <div class="citation-container display-none" id="verified-citation-apa">
                <button class="copy-btn" aria-label="Copy citation">Copy</button>
                <pre>Jimenez, C. E., Yang, J., Wettig, A., Yao, S., Pei, K., Press, O., & Narasimhan, K. R. (2024). SWE-bench: Can Language Models Resolve Real-world Github Issues? arXiv preprint arXiv:2310.06770.</pre>
            </div>

            <div class="citation-container display-none" id="verified-citation-mla">
                <button class="copy-btn" aria-label="Copy citation">Copy</button>
                <pre>Jimenez, Carlos E., et al. "SWE-bench: Can Language Models Resolve Real-world Github Issues?" arXiv preprint arXiv:2310.06770 (2024).</pre>
            </div>
        </section>
    </div>
{% endblock %}

{% block scripts_extra %}
{% endblock %}
